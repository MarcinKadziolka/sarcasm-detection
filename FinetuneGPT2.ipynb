{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU",
    "gpuClass": "standard"
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "!pip install -q git+https://github.com/huggingface/transformers.git\n",
        "!pip install -q git+https://github.com/gmihaila/ml_things.git"
      ],
      "metadata": {
        "id": "Tw0bE78xF_Rj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import io\n",
        "import os\n",
        "import torch\n",
        "from tqdm.notebook import tqdm\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "from ml_things import plot_dict, plot_confusion_matrix, fix_text\n",
        "from sklearn.metrics import classification_report, accuracy_score\n",
        "from transformers import set_seed, TrainingArguments, Trainer, GPT2Config, GPT2Tokenizer, AdamW, get_linear_schedule_with_warmup, GPT2ForSequenceClassification\n",
        "\n",
        "epochs = 7\n",
        "batch_size = 32\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "labels_ids = {0: 0, 1: 1}\n",
        "n_labels = len(labels_ids)\n"
      ],
      "metadata": {
        "id": "R_9i7QKcGFHf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "class SarcasmDataset(Dataset):\n",
        "  def __init__(self, df):\n",
        "    self.texts = []\n",
        "    self.labels = []\n",
        "    for index, row in df.iterrows():\n",
        "      content = fix_text(row['headline'])\n",
        "      self.texts.append(content)\n",
        "      self.labels.append(row['is_sarcastic'])\n",
        "\n",
        "    self.n_examples = len(self.labels)\n",
        "        return\n",
        "\n",
        "  def __len__(self):\n",
        "    return self.n_examples\n",
        "\n",
        "  def __getitem__(self, item):\n",
        "    return {'text':self.texts[item],\n",
        "            'label':self.labels[item]}\n",
        "\n",
        "\n",
        "\n",
        "class Gpt2ClassificationCollator(object):\n",
        "    def __init__(self, tokenizer, labels_encoder):\n",
        "        self.tokenizer = tokenizer\n",
        "        self.max_sequence_len = tokenizer.model_max_length\n",
        "        self.labels_encoder = labels_encoder\n",
        "\n",
        "        return\n",
        "\n",
        "    def __call__(self, sequences):\n",
        "        texts = [sequence['text'] for sequence in sequences]\n",
        "        labels = [sequence['label'] for sequence in sequences]\n",
        "        labels = [self.labels_encoder[label] for label in labels]\n",
        "        inputs = self.tokenizer(text=texts, return_tensors=\"pt\", padding=True, truncation=True,  max_length=self.max_sequence_len)\n",
        "        inputs.update({'labels':torch.tensor(labels)})\n",
        "\n",
        "        return inputs\n",
        "\n",
        "\n",
        "def train(model, dataloader, optimizer_, scheduler_, device_):\n",
        "\n",
        "  predictions_labels = []\n",
        "  true_labels = []\n",
        "  total_loss = 0\n",
        "  model.train()\n",
        "\n",
        "  for batch in tqdm(dataloader, total=len(dataloader)):\n",
        "\n",
        "    true_labels += batch['labels'].numpy().flatten().tolist()\n",
        "    batch = {k:v.type(torch.long).to(device_) for k,v in batch.items()}\n",
        "\n",
        "    model.zero_grad()\n",
        "    outputs = model(**batch)\n",
        "    loss, logits = outputs[:2]\n",
        "    total_loss += loss.item()\n",
        "    loss.backward()\n",
        "    torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)\n",
        "    optimizer.step()\n",
        "    scheduler.step()\n",
        "    logits = logits.detach().cpu().numpy()\n",
        "    predictions_labels += logits.argmax(axis=-1).flatten().tolist()\n",
        "\n",
        "  avg_epoch_loss = total_loss / len(dataloader)\n",
        "  return true_labels, predictions_labels, avg_epoch_loss\n",
        "\n",
        "\n",
        "\n",
        "def validate(model, dataloader, device_):\n",
        "  predictions_labels = []\n",
        "  true_labels = []\n",
        "  total_loss = 0\n",
        "  model.eval()\n",
        "\n",
        "  for batch in tqdm(dataloader, total=len(dataloader)):\n",
        "\n",
        "    true_labels += batch['labels'].numpy().flatten().tolist()\n",
        "\n",
        "    batch = {k:v.type(torch.long).to(device_) for k,v in batch.items()}\n",
        "\n",
        "    with torch.no_grad():\n",
        "\n",
        "        outputs = model(**batch)\n",
        "        loss, logits = outputs[:2]\n",
        "        logits = logits.detach().cpu().numpy()\n",
        "        total_loss += loss.item()\n",
        "        predict_content = logits.argmax(axis=-1).flatten().tolist()\n",
        "        predictions_labels += predict_content\n",
        "\n",
        "  avg_epoch_loss = total_loss / len(dataloader)\n",
        "  return true_labels, predictions_labels, avg_epoch_loss\n"
      ],
      "metadata": {
        "id": "YGZGFDUrGLep"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model_config = GPT2Config.from_pretrained(pretrained_model_name_or_path='gpt2', num_labels=n_labels)\n",
        "\n",
        "tokenizer = GPT2Tokenizer.from_pretrained(pretrained_model_name_or_path='gpt2')\n",
        "tokenizer.padding_side = \"left\"\n",
        "tokenizer.pad_token = tokenizer.eos_token\n",
        "model = GPT2ForSequenceClassification.from_pretrained(pretrained_model_name_or_path='gpt2', config=model_config)\n",
        "\n",
        "model.resize_token_embeddings(len(tokenizer))\n",
        "\n",
        "model.config.pad_token_id = model.config.eos_token_id\n",
        "\n",
        "model.to(device)\n"
      ],
      "metadata": {
        "id": "L4cZ_4YWGVt4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "gpt2_classificaiton_collator = Gpt2ClassificationCollator(tokenizer=tokenizer, labels_encoder=labels_ids)\n",
        "\n",
        "df = pd.read_json('Sarcasm_Headlines_Dataset_v2.json', lines = True)\n",
        "train_df, test_df = train_test_split(df, test_size=0.2)\n",
        "train_dataset = SarcasmDataset(train_df)\n",
        "train_dataloader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True, collate_fn=gpt2_classificaiton_collator)\n",
        "\n",
        "valid_dataset =  SarcasmDataset(test_df)\n",
        "valid_dataloader = DataLoader(valid_dataset, batch_size=batch_size, shuffle=False, collate_fn=gpt2_classificaiton_collator)\n"
      ],
      "metadata": {
        "id": "dZJ4KaiSGduC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "optimizer = AdamW(model.parameters(), lr = 2e-5, eps = 1e-8)\n",
        "scheduler = get_linear_schedule_with_warmup(optimizer,\n",
        "                                            num_warmup_steps = 0,\n",
        "                                            num_training_steps = len(train_dataloader) * epochs)\n",
        "\n",
        "all_loss = {'train_loss':[], 'val_loss':[]}\n",
        "all_acc = {'train_acc':[], 'val_acc':[]}\n",
        "\n",
        "\n",
        "for epoch in tqdm(range(epochs)):\n",
        "  train_labels, train_predictions, train_loss = train(model, train_dataloader, optimizer, scheduler, device)\n",
        "  train_acc = accuracy_score(train_labels, train_predictions)\n",
        "  valid_labels, valid_predictions, val_loss = validate(model, valid_dataloader, device)\n",
        "  val_acc = accuracy_score(valid_labels, valid_predictions)\n",
        "  all_loss['train_loss'].append(train_loss)\n",
        "  all_loss['val_loss'].append(val_loss)\n",
        "  all_acc['train_acc'].append(train_acc)\n",
        "  all_acc['val_acc'].append(val_acc)\n",
        "\n",
        "plot_dict(all_loss, use_xlabel='Epochs', use_ylabel='Value')\n",
        "\n",
        "plot_dict(all_acc, use_xlabel='Epochs', use_ylabel='Value')\n"
      ],
      "metadata": {
        "id": "ttTaVGiBGh9E"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "true_labels, predictions_labels, avg_epoch_loss = validation(valid_dataloader, device)\n",
        "evaluation_report = classification_report(true_labels, predictions_labels)\n",
        "plot_confusion_matrix(y_true=true_labels, y_pred=predictions_labels, classes=list(labels_ids.keys()), normalize=True, magnify=0.1)"
      ],
      "metadata": {
        "id": "w_E3ExhRGlgA"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}